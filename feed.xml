<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://fszewczyk.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fszewczyk.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-01T17:12:44+00:00</updated><id>https://fszewczyk.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Predicting Physics Using Graph Networks and Video Embeddings</title><link href="https://fszewczyk.github.io/blog/2024/predicting-physics-using-graph-networks-and-video-embeddings/" rel="alternate" type="text/html" title="Predicting Physics Using Graph Networks and Video Embeddings"/><published>2024-04-21T17:19:49+00:00</published><updated>2024-04-21T17:19:49+00:00</updated><id>https://fszewczyk.github.io/blog/2024/predicting-physics-using-graph-networks-and-video-embeddings</id><content type="html" xml:base="https://fszewczyk.github.io/blog/2024/predicting-physics-using-graph-networks-and-video-embeddings/"><![CDATA[<p>Why do we need to predict physical behavior? Let’s say you want to build a boat. Turns out, it’s much cheaper to validate your boat design before you actually build the boat. To do that, you should check how your invention behaves in the destination environment before the actual manufacturing. You can do that using computer simulations which approximate the physical interactions between the water and your boat.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*nlD46QflWlWqBO0ATUdIQg.gif"/><figcaption>Visualization of fluid dynamics simulation</figcaption></figure> <p>For a more trivial example, we can look at video games. The game below, Sea Of Thieves, is known to have one of the most realistic representations of water in the whole game industry. In this case, the simulation can be less accurate at the expense of performance since you would want the game to run as fast and smoothly as possible.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/480/1*nOl35sOMkQVZV7AP3PUCSA.gif"/><figcaption>Screenshot from “Sea of Thieves”</figcaption></figure> <p><em>Before we continue, here is a small disclaimer: This post is a transcript of the presentation I gave during the symposium at the University of Groningen. The associated thesis will be linked here very soon. I hope, that this post provides a brief introduction to Graph Network-based Simulators and shows how I have extended this method so that it can infer the trajectory of any system based on a short video.</em></p> <p>So, how do you do that? These are the Navier-Stokes Equations. They describe the motion of viscous fluid substances and can be used to describe the water flow in a pipe or an air flow around a wing of a plane. If you happen to accidentally find the solutions to them in three dimensions, hit me up, because there’s a million dollars prize if you do. So in short, math is hard. Fortunately, there are ways to approximate fluid motion without calculating this monster.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZagGShhfPTEs5jMAm9vRRQ.png"/><figcaption>Navier-Stokes Equations</figcaption></figure> <p>And these simulations are fast. What you see here is a simulation method called Smoothed Particle Hydrodynamics. Particles affect the other particles nearby. Describing these local behaviors and deploying them on thousands of particles can yield very accurate results. Let’s try to figure out how to build a neural network that predicts this behavior.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*SPp-oAzQX9t7oGXWoRl2-Q.gif"/><figcaption>SPH Simulation. Source: <a href="https://www.reddit.com/r/Python/comments/fn5449/a_2d_fluid_simulation_using_sph_and_gpu/"><em>https://www.reddit.com/r/Python/comments/fn5449/a_2d_fluid_simulation_using_sph_and_gpu/</em></a></figcaption></figure> <p>To do so, we need to frame the problem more explicitly. The input is a set of particles. Then, this state should pass through some funny function to come up with new positions of these particles. Repeat this process million times and we have a full simulation. Pretty easy.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1022/1*9Py98J-SY1ucrsqdn2q9xA.png"/><figcaption>Based on: <a href="https://arxiv.org/pdf/2002.09405.pdf">Learning to simulate complex physics using Graph Networks</a></figcaption></figure> <p>Guys at DeepMind would fully agree with that. They came up with a method that can predict the behavior of complex physical systems. One of the videos you see is the predicted trajectory, while the other one was simulated using traditional methods. Can you guess what is what?</p> <p>Well, as you can see, people at DeepMind kind of know what they are doing.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*X3Tk39Lqq6P6K_3q4JC0AQ.png"/></figure> <p>Ok, but what if you want to predict the motion of sand? Then, you need to retrain a new model from scratch on a new dataset.</p> <p>What if you wanna have a model that’s specialized in goop? Then again, new dataset, new training, new model.</p> <p>What about something in between? What about a model that predicts goop-o-sand behavior? Again, it needs to be trained from scratch. Even if you already have a model of sand and a model of goop!</p> <p>That brings us to our research question: <strong>does a video of a physical system contain enough information to generalize Graph Networks?</strong></p> <p>We want to build a system that based on a single video, can predict the behavior of the system visible in the video. Ideally, the whole system would require only a single training procedure and it would be able to generalize well across different systems.</p> <p>So here’s my suggestion. We are going to build a model. This model is going to watch a video of a physical system and predict its properties in some latent space. This would allow to encode the physical properties of any system while preserving any differences between them. As you can see, the goop-o-sand lies somewhere in between sand and goop indicating that its behavior should be an “average” of these two.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ygd7c1spyokYQfGU0kScQQ.png"/><figcaption>Visualization of physical encodings of different systems. From the top: sand, water, goop, sand-o-goop</figcaption></figure> <p>So let’s use this physical encoding to predict the system’s motion. Firstly, the Video Encoder watches a video and calculates the physical properties of the system. This encoding is going to differ based on the video that the it watched.</p> <p>Then, this encoding is combined with the information from the original particles.</p> <p>Firstly, we create a graph out of the original particles. Each particle becomes a graph vertex. The edges are created between the neighbouring particles. Each edge contains information about the displacement between the particles it connects. Each particles stores its last three velocities as well as the physical properties provided by the Video Encoder.</p> <p>Now, both edges and vertices store vectors. We can pass these vectors through their respective encoders to obtain a latent graph. The encoded graph has exactly the same structure as the original one, but the encodings of edges and vertices have been updated using these small neural networks.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*VYTJoqEtkxUfj1GS_FEiKA.png"/><figcaption>Graph Encoding</figcaption></figure> <p>Perfect! Now, every edge has some encoding and every vertex has some other encoding. We are going to perform a procedure called message passing. In this example, we are going to focus on the purple node.</p> <p>First, we find the three edges that are connected to the purple node. Let’s focus on the edge that has the E-Zero encoding. We’re going to construct a vector in the following way. Since every edge always connects exactly two nodes, we are going to take the encodings of these two nodes. In this case, V-0 and V-1. Additionally, we are also going to include the encoding of the edge itself. We repeat this procedure for every edge connected to the purple node.</p> <p>These vectors are then passed through yet another neural network and then summed together.</p> <p>This output finally goes through one more small module in order to obtain new node encodings. The output of this procedure is fed back into the original node. This whole procedure is then repeated again and again. In my experiments, I performed three of these message-passing steps.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*rYTeXPR0xfQS9205HwCa6Q.png"/><figcaption>Graph Processor</figcaption></figure> <p>Perfect! Now the last step. Each particle has an associated vector with its encoding.</p> <p>We can finally decode the graph to finally obtain the acceleration of each individual particle. Since we have the acceleration, we can calculate the new velocity. With the new velocity, we can calculate the new positions for each individual particle. Now, we can repeat this whole procedure to get the state of the system at the next timestep.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*kbnnEH7QcA0qfgdkvVMFEA.png"/><figcaption>Graph Decoder</figcaption></figure> <p>Let’s test it out. Before jumping into fluids, I tested the approach I just described on a simpler system. The system consists of 10 particles, connected with springs. This setup is naturally converted into a graph where the springs are simply replaced with edges. Also, since the systems differ only in one value k, which is the spring constant, the physical encoding calculated from the video contains only a single number.</p> <p>To train the model, I generated trajectories for systems with varying spring constants. On the right, you can see 5 different classes of systems. During my experiments, I varied the amount of system classes. Some models are trained only using trajectories with either very low or very high spring constants, while some are trained with even 10 distinct system classes.</p> <p>In total, every dataset always contains 320 trajectories. Each trajectory is 3 seconds long, totaling 300 timesteps. Further, each sample with the graph has an associated video of the trajectory.</p> <p>During the training, samples are chosen in a non-standard manner to increase the variance in the dataset. Firstly, a single timestep is chosen. This contains the input graph as well as the target accelerations. Let’s say this timestep is from a class where the spring constant k is equal to 4000. Then, out of all the videos of systems with that particular spring constant, a random one is chosen. This video is then passed to the network together with the graph in order to calculate the accelerations.</p> <p>What’s important is that the video encoder is trained together with the graph network at the same time. This means that the video encoder is learning the physical properties solely based on the predicted accelerations.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/614/1*5PTOiyBzEvro51XSlUIAVw.gif"/><figcaption>Visualization of trajectories with different spring constants</figcaption></figure> <p>And here are the results! Based on the video, the physical encoding is different. On top, you can see a video of a system with a low spring constant and its low physical encoding. On the bottom is a video of a system with stiffer springs, resulting in a high physical encoding.</p> <p>On the right are the predicted trajectories. What’s interesting is that both of them started with the exact same initial condition. However, based on the physical encoding, my model managed to generate rollouts that resemble the system visible on the video. Springs in the top-right are much more flexible than the ones in the bottom-right. Before calling it a success, let’s look at some more data</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*uvOtwwdd4x4gMOytK-e8kQ.gif"/><figcaption>Predicted physical encodings and rollout trajectories</figcaption></figure> <p>Firstly, the model that was trained using only the trajectories of two systems, either with a very low spring constant or a very high spring constant. While the one-step errors for these two classes are very low, the errors are way larger for the systems that are somewhat in between. That’s likely because the model has only seen those extreme cases, hence it cannot interpolate that easily between them.</p> <p>If the model is trained with 5 different classes, a similar phenomenon occurs, the errors are low for the systems present during training and higher for the ones that the model has never seen.</p> <p>Finally, the errors are the lowest if the model has seen 10 different systems with 10 different spring constants. These results align with the intuition of neural networks since they do a very good job when dealing with data present in the original distribution but have trouble going out of distribution.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9XUrISX8VEs_FXaTUbVEYw.png"/><figcaption>Mean Euclidean distances between true and predicted accelerations for systems with varying spring constant</figcaption></figure> <p>Another interesting thing I wanna show you are the physical encodings. In the first model, the mean encoding of various systems seems to clearly correspond to the true spring constant.</p> <p>Similar things can be said about the other models. There is also a clear correspondence between the true spring constant the the predicted video encoding.</p> <p>What’s interesting now, is the variance of these encodings. The highlighted region represents the standard deviation of the results. As you can see, the variance is low for the systems that are present in the original distribution and is quite large for the systems that were not present in the training dataset.</p> <p>For the model trained with 5 classes, the variance is visibly smaller indicating that the model is more “confident” in its predictions.</p> <p>This effect is even larger for the final model trained on 10 system classes.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*TLbDVsQWjDSajThbY6d5HA.png"/><figcaption>Video Encodings predicted by models trained on different number of classes of physical systems</figcaption></figure> <p>Based on what we know let’s answer the original question. Can we use a short video to reliably encode physical properties of the system?</p> <p>Yes, we can. Can we train the whole system in one single procedure?</p> <p>Yes, we can. Finally, can we reliably interpolate between different materials?</p> <p>Kind of, as we have seen, models struggle generalizing outside of the distribution. However, as the training distribution becomes more granular and contains more variation in the system classes, the errors become way smaller across the spectrum.</p> <p>Thank you for reading through this blog post. It would mean a lot if you let me know what you think about my work! Cheerios :)</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f485b168c49d" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Generating Synthetic Dataset for Satelite Imaging</title><link href="https://fszewczyk.github.io/blog/2024/generating-synthetic-dataset-for-satelite-imaging/" rel="alternate" type="text/html" title="Generating Synthetic Dataset for Satelite Imaging"/><published>2024-02-24T17:38:21+00:00</published><updated>2024-02-24T17:38:21+00:00</updated><id>https://fszewczyk.github.io/blog/2024/generating-synthetic-dataset-for-satelite-imaging</id><content type="html" xml:base="https://fszewczyk.github.io/blog/2024/generating-synthetic-dataset-for-satelite-imaging/"><![CDATA[<figure><img alt="Robot taking aerial pictures of sea vessels and ships" src="https://cdn-images-1.medium.com/max/1024/1*IYXWUNvHov3H9HBI5TpMRA.png"/></figure> <h3>What is synthetic data?</h3> <p>In the realm of artificial intelligence and computer vision, access to diverse and annotated datasets is crucial for training robust models. However, gathering such datasets, especially in specialized fields like medical imaging, document recognition or satellite imaging presents challenges due to privacy constraints and limited availability. Synthetic data addresses these hurdles by providing a means to generate vast and varied datasets, mimicking real-world scenarios with fidelity. The use of synthetic data lowers the cost of data collection and lifts the burdens of manual data annotation. In this blog post, I will use <a href="https://rodina.ai">Rodina Flow</a> to generate a high-quality, synthetic dataset with ready-to-go annotations.</p> <h3>Detecting Ships from Aerial Images</h3> <p>In particular, we will focus on an object detection task in the context of satellite imaging. On <a href="https://kaggle.com">Kaggle</a>, you can find a lot of varied datasets for computer vision models. In this example, we will be using a dataset containing almost 27,000 annotated samples of aerial images of ships and other sea vessels. You can access this dataset <a href="https://www.kaggle.com/datasets/siddharthkumarsah/ships-in-aerial-images">here</a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/794/1*-eOwfFSvNV4DKqg8c9TmfQ.png"/><figcaption>Annotated sample from the original dataset</figcaption></figure> <h3>Preparing the data</h3> <p>This dataset contains various types of images, ranging from close-ups to distant shots, with varying degrees of quality. To streamline our analysis, we will concentrate on a specific subset of images that depict ships from a considerable distance. Notably, we will select a subset of samples with prefixes: <em>SA_, s, PE_, OG_, m, and GE_. </em>This amounts to 3289 samples. We select 500 to be in the test set, which leaves 2789 training examples. We will use those to establish the maximum performance of the model if it was trained on the whole data. We also randomly selected 150 images that we will use to synthesize our dataset. In the next post, I will evaluate the performance of models trained on real and synthetic data in an object detection task. This will allow us to check how good is the synthetic data we have generated. The rest of this post will be a tutorial on synthetic image generation using <a href="https://rodina.ai"><em>Rodina Flow</em></a><em>.</em></p> <p>After opening the program, you can see two nodes. One of them reads the data from the disk, while the other one saves the results to your filesystem.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*iv0-Oy0SwiKAXolfbYUdWQ.png"/><figcaption>Starting setup of Rodina Flow</figcaption></figure> <p>Let’s start by clicking on the <em>Dataset Settings </em>button on the left node. Here, you should specify the directory containing the images. In our case, those images are located in the directory stored as synthetic/source/. In the top bar, we also need to select the directory containing the annotations. In our case, the annotations are stored in the YOLO format and are located in the labels/ directory. If everything goes well, you should see a preview of your dataset.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OOFVhyoFOrwh-hB7XoTyHA.png"/><figcaption>Preview of the loaded dataset with the bounding box annotations</figcaption></figure> <p>As you can see, the images are appropriately loaded. On the right toolbar, you can see that the image contains three annotations for <em>Class 0</em>. This class indicated the YOLO class with an identifier of 0. In our case, this identifier represents a ship.</p> <p>If we exit this view by pressing the icon in the top-right corner, we can also preview the sample by hovering over the icon of the <em>eye </em>on the node.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8k5LpQCuteTthEYsmtV63Q.png"/><figcaption>You can preview the output of each node by hovering over this icon. The pink outline shows the annotations.</figcaption></figure> <h3>Building the workflow</h3> <p>Before we get to it, let’s first think about our goals. We want to:</p> <ol><li>Create as many samples as possible</li><li>Introduce a lot of variance within the dataset</li><li>Make sure the samples are realistic</li></ol> <p>Let’s address these goals one by one. To achieve a large dataset, we will combine two images. One will act as a background and one will act as a foreground. If we cut out the ships from the foreground and paste them onto the background, we will end up with a completely new example.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*r4WmzsBRIAWHRoCieDQlDA.png"/><figcaption>Two Samples are composited together to create a new, annotated Sample</figcaption></figure> <p>To do so, we need to add one more node that reads the background image. We head to the menu in the top-left corner and simply drag and drop it in the desired location.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YKeoxL7YX9iapAl9OCH9nw.gif"/><figcaption>You can add a new node by clicking it in the menu.</figcaption></figure> <p>To address the issue of variance, we will randomly modify the samples through a simple image augmentation process. To do that, we will resize the image to a pre-defined size and then shift its hue, adjust contrast, and add some blur. We can easily make sure that these augmentations are applied with varied strengths using nodes that generate random numbers.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*kM1ps-B3QWrJSLcKjHNXQA.png"/><figcaption>The image undergoes modifications that increase the variance of the dataset. Note that the image properties are modified with the use of random numbers sampled either from a Gaussian Distribution or a Uniform Distribution.</figcaption></figure> <p>When it comes to the foreground (cut-out) ship, we need to perform some more advanced processing. First of all, we normalize the size to a standard one and slightly blur the image (again, using a random blur size). <em>Rodina Flow</em> provides a node called “Cut-out Box” which cuts out the bounding box with a specified class. This results in a sample where only the ships are visible, while the rest of the image (the ocean) is transparent.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Yh_5nLtXMCqB_n77NOMvfA.png"/><figcaption>Multiple cutout ships viewed from a large distance. The pink outline indicated the bounding box annotations.</figcaption></figure> <p>Finally, we can crop this image to content. This will remove the transparent borders such that the sample is fully contained. Before stacking the images on top of each other, we need to make sure they are the same size. To do so, we will add a transparent border to the sample, such that the result has the same dimensions as the background. We will also flip the sample with a probability of 50%. If you want to specify the execution probability of a certain node, you can click the button in the top-left corner of the node.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*kKqRvhdm9T9Aot_cWu3M1Q.png"/><figcaption>Half of the Samples are flipped vertically and horizontally.</figcaption></figure> <p>Now, we can composite these two images together. To address the final goal of realism, we will use the <em>Poisson Blend</em> node, which uses optimization algorithms to seamlessly blend the foreground and the background. Our final workflow is presented as follows.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*lgJH-2C5GO-CCaqZutGg-Q.png"/><figcaption>Synthetic generation pipeline. You can follow each image processing step by tracing the path all the way from the <em>Read Nodes to the Write Node</em></figcaption></figure> <p>You might notice that there is a single, green node on the right. This is the <em>Write</em> Node, which saves the resulting dataset to your filesystem. It’s possible to choose a format of the dataset. While <em>Rodina Flow</em> supports multiple dataset structures, we will opt for YOLO.</p> <h3>Dataset Generation and Results</h3> <p>We can preview the created sample by hovering over the <em>eye</em> icon in the <em>Poisson Blend</em> node. If we click it, the pipeline will be re-run with new inputs resulting in a completely new sample.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MjWYAmARY3goeRhCj0l1EA.png"/><figcaption>It is difficult to tell that the ship in the middle has been artificially pasted.</figcaption></figure> <p>This looks very good! Let’s run the pipeline, by Clicking the <em>Run</em> button at the top of the screen.</p> <p>Our source dataset contains 150 images. <em>Rodina Flow</em> will combine every foreground with every background resulting in a total of 150*150=22500 annotated samples. Let’s look at some of the results.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MMiQHKjpgD2TrnSoqmdDDw.png"/><figcaption>Marked ships have not been there in the original dataset.</figcaption></figure> <p>WOW! They look incredibly good. It’s close to impossible to tell which ships have been artificially pasted. In the next blog, we will see how useful this data is.</p> <p>Unfortunately, some of the examples look quite goofy, but let’s not worry about it since they constitute a small minority :)</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*42EGC8e363RufOm-gPyfEQ.png"/><figcaption>A small minority of Samples do not look realistic enough</figcaption></figure> <h3>Conclusion</h3> <p>Synthetic data is a powerful tool in areas where the usage of real datasets is impossible due to privacy constraints or simply lack of high-quality data itself. Furthermore, research shows that it can be used to achieve extremely accurate computer vision models.</p> <p><a href="https://rodina.ai"><em>Rodina Flow</em></a> provides an easy way to generate synthetic image datasets through image composition. It also provides multiple options for image augmentations. I hope you enjoyed this walkthrough and that you learned something. If you want to see even more, check out <a href="https://rodina.ai/blog">our blog</a>, and in case you have any questions or feedback, please leave a comment or <a href="https://www.rodina.ai/about#-talk-to-us-">reach out to us directly</a>. Thanks for sticking around and have a wonderful day!</p> <p><strong><em>Disclaimer: I am using Rodina Flow to generate the dataset in this blog post. I am one of the creators of this software. I want to demonstrate how easy it is to generate artificial high-quality datasets with a minimal amount of initial data.</em></strong></p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=c9e1cefb175b" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry><entry><title type="html">Rope Simulator in C++</title><link href="https://fszewczyk.github.io/blog/2023/rope-simulator-in-c/" rel="alternate" type="text/html" title="Rope Simulator in C++"/><published>2023-09-11T21:12:45+00:00</published><updated>2023-09-11T21:12:45+00:00</updated><id>https://fszewczyk.github.io/blog/2023/rope-simulator-in-c</id><content type="html" xml:base="https://fszewczyk.github.io/blog/2023/rope-simulator-in-c/"><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*l1i7Gfh5q1wUPq15"/><figcaption>Photo by <a href="https://unsplash.com/@meganmenegay?utm_source=medium&amp;utm_medium=referral">Megan Menegay</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure> <p>For some time now, I’ve been missing quick and to-the-point tutorials on particle-based physics simulations. Hence, I decided to start sharing things I learned in a concise manner. For starters, we’re going to implement a simple rope simulator. However, before we do anything, a quick disclaimer.</p> <p><em>This tutorial is aimed at achieving rapid, visual results. I do not follow perfect software engineering practices and do not explain the math in-depth.</em></p> <p>Now that you’ve been warned, let’s get to it!</p> <h3>What is a rope?</h3> <p>What rope is, everybody sees. In order to create a digital representation of one, we divide it into multiple small segments, connected by points. Since rope and spring are two different things, those segments have equal and constant lengths — <em>D</em>.</p> <figure><img alt="Process of splitting a rope into equal segments in order to create its digital representation" src="https://cdn-images-1.medium.com/max/980/1*OOl6Q55HxhJ1lqCfqiGHWg.png"/><figcaption>Splitting a rope into equal segments</figcaption></figure> <p>This representation lets us focus on individual points (or particles) and their properties.</p> <h3><strong>A bit of math</strong></h3> <p>Now that we have the representation of a rope, how can we simulate it? First, we need to update the positions of each point based on the forces applied to the rope, such as gravity. Then, we need to make sure that our rope does not stretch or compress.</p> <h4>Verlet Integration</h4> <p>You’re probably familiar with the simplest of all — Euler integration. At a certain timestep, you update velocity based on acceleration and then update position based on velocity. We need to define a step size, for example, 0.01 seconds. In a more formal way, this method looks like this.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/608/1*e_EIBfMeLTPGhM6OGoJAuA.png"/></figure> <figure><img alt="" src="https://cdn-images-1.medium.com/max/682/1*N0Tt63a0XmibJZlpT78LGQ.png"/></figure> <p>Everything looks great, but over time we lose a lot of precision, and the accuracy of the simulation is highly dependent on the chosen timestep. If you wanna see more details, please see <a href="https://owlree.blog/posts/simulating-a-rope.html">this post</a>. This issue brings us to Verlet integration, which is more stable and accurate. The basic idea is the following, we update our position based on the previous and instant velocity. I won’t derive the whole formula, cause it takes a bit of time to get right. In short, it looks like this.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/888/1*CozwbUSR-mV4581eRPuLbA.png"/></figure> <p>After we repeat this procedure for every particle, we have successfully calculated the new state of the rope at the next time step. However, there is a problem. We might end up in a situation, where segments have different lengths. This can happen whenever two opposing forces are acting on neighboring particles. Let’s enforce this constraint!</p> <h4>Enforcing constraints</h4> <p>After the Verlet integration, segments may be slightly shorter or longer than before. In the beginning, we defined <em>D </em>to be our target segment length. This means, that if our rope consists of 10 particles, 9 segments, and <em>D=1</em>, then the rope should be 9 units long <strong>at all times</strong>. To make sure this is the case, we use the Jakobsen method. This method is used whenever a constraint of fixed distance between particles needs to be enforced, so feel free to use it in other projects, such as cloth simulations!</p> <p>Let’s get to the algorithm. We take two particles <em>(p,q)</em> connected by a segment of length <em>d=||p-q||</em>. If <em>d</em> is smaller than the desired distance <em>D</em>, we push particles apart, each by <em>0.5(D-d)</em>. If they are too far, we do the opposite, we pull them closer, each by <em>0.5(d-D)</em>. Tricky things happen when a certain point is fixed. Think of it, as if the rope was pinned to the wall throughout the simulation. Then, we cannot move this particle around while enforcing constraint, hence we drop the <em>0.5</em> factor and move the neighboring particle by <em>|d-D|</em> in the appropriate direction. By repeating this procedure multiple times, our segments should have approximately appropriate lengths.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*CwHxvmnBl5-ylr2z1bm5_A.png"/><figcaption>A single iteration of enforcing constraints. Note that the particle at the top is fixed</figcaption></figure> <h3>Code</h3> <p><a href="https://gist.github.com/fszewczyk/46915c02a34a1833d83a3c2fd851b7a0"><em>Available on GitHub</em></a></p> <p>Finally, we are ready to code it up. I will do it in C++, but feel free to use whatever you like. The translation should be rather easy.</p> <p>First and foremost, let’s import the necessary libraries and put some global constants in place.</p> <pre>#include &lt;math.h&gt;<br />#include &lt;stdlib.h&gt;<br />#include &lt;vector&gt;<br /><br />float GRAVITY = -9.81;             // constant force applied to all particles<br />float TIMESTEP = 0.01;             // Δt in our equations<br />unsigned JAKOBSEN_ITERATIONS = 50; // Number of times we will enforce the distance constraint</pre> <p>For convenience, let’s define a particle. Remember, we need to keep track of its previous position in order to perform Verlet integration.</p> <pre>struct Particle {<br />    // Current position<br />    float x;<br />    float y;<br /><br />    // Previous position<br />    float xPrevious;<br />    float yPrevious;<br /><br />    // Whether or not particle is able to move<br />    bool fixed;<br />};</pre> <p>We will enclose the whole functionality in a class called Rope. I’ll define a simple constructor that creates a rope between two points. The first point will be fixed, while all others not. Depending on the application, we also want to control the total number of particles in a rope.</p> <pre>class Rope {<br />  public:<br />    Rope(float x1, float y1, float x2, float y2, unsigned nParticles) {<br />        for (unsigned i = 0; i &lt; nParticles; i++) {<br />            // How close are we to the last point?<br />            float w = (float)i / (nParticles - 1);<br /><br />            float x = w * x2 + (1 - w) * x1;<br />            float y = w * y2 + (1 - w) * y1;<br /><br />            Particle p;<br />            p.x = x;<br />            p.y = y;<br />            p.xPrevious = x;<br />            p.yPrevious = y;<br />            p.fixed = i == 0; // We fix only the first point<br /><br />            _particles.push_back(p);<br />        }<br /><br />        // There is one less segment than there are particles<br />        unsigned numberOfSegments = nParticles - 1;<br />        float ropeLength = sqrt(pow(x1 - x2, 2) + pow(y1 - y2, 2));<br /><br />        _desiredDistance = ropeLength / numberOfSegments;<br />    }<br /><br />  private:<br />    // We need to store our particles somewhere<br />    std::vector&lt;Particle&gt; _particles;<br /><br />    // Target distance of a single segment<br />    float _desiredDistance;<br />};</pre> <p>To advance the simulation by one step, we define a public function step. As we discussed, a single step consists of a Verlet integration and Jakobsen constraint enforcement.</p> <pre>void step() {<br />    verletIntegration();<br />    enforceConstraints();<br />}</pre> <p>Let’s define a private method verletIntegration, that will take care of the fancy math.</p> <pre>void verletIntegration(){<br />    for (Particle &amp;p : _particles){<br />        if(p.fixed)<br />            continue; // We do not want to move fixed particles<br />        float xCopy = p.x;<br />        float yCopy = p.y;<br /><br />        // Updating particle&#39;s position<br />        p.x = 2 * p.x - p.xPrevious + 0 * TIMESTEP * TIMESTEP;<br />        p.y = 2 * p.y - p.yPrevious + GRAVITY* TIMESTEP * TIMESTEP;<br /><br />        p.xPrevious = xCopy;<br />        p.yPrevious = yCopy;<br />    }<br />}</pre> <p>Perfect! Let’s enforce those constraints. We define another private method enforceConstraints.</p> <pre>void enforceConstraints(){<br />    // We perform the enforcement multiple times<br />    for (unsigned iteration = 0; iteration &lt; JAKOBSEN_ITERATIONS; iteration++){<br />        // We iterate over each pair of pa<br />        for (size_t i = 1; i &lt; _particles.size(); i++) {<br />            Particle &amp;p1 = _particles[i-1];<br />            Particle &amp;p2 = _particles[i];<br />            <br />            // Calculating distance between the particles<br />            float distance = sqrt(pow(p1.x - p2.x, 2) + pow(p1.y - p2.y, 2));<br />            float distanceError = distance - _desiredDistance;<br /><br />            // The direction in which particles should be pulled or pushed<br />            float xDifference = p2.x - p1.x;<br />            float yDifference = p2.y - p1.y;<br /><br />            // We need to make it a unit vector<br />            // This will allow us to easily scale the impact we have<br />            // on each particle&#39;s position.<br />            float xDirection = xDifference / sqrt(pow(xDifference, 2) + pow(yDifference, 2));<br />            float yDirection = yDifference / sqrt(pow(xDifference, 2) + pow(yDifference, 2));<br /><br />            // Finally, we can update particles&#39; positions<br />            // We need to remember that fixed particles should stay in place<br />            if (p1.fixed &amp;&amp; !p2.fixed) {<br />                p2.x -= xDirection * distanceError;<br />                p2.y -= yDirection * distanceError;<br />            } else if (p2.fixed &amp;&amp; !p1.fixed) {<br />                p1.x += xDirection * distanceError;<br />                p1.y += yDirection * distanceError;<br />            } else if (!p1.fixed &amp;&amp; !p2.fixed){<br />                p2.x -= 0.5 * xDirection * distanceError;<br />                p2.y -= 0.5 * yDirection * distanceError;<br />                p1.x += 0.5 * xDirection * distanceError;<br />                p1.y += 0.5 * yDirection * distanceError;<br />            }<br />        }<br />    }<br />}</pre> <p>That’s it! Please see the <a href="https://gist.github.com/fszewczyk/46915c02a34a1833d83a3c2fd851b7a0">GitHub Gist</a> for the whole code. Our simulator is ready to be used. To show its functionality, I implemented a Python interface using <a href="https://pybind11.readthedocs.io/en/stable/index.html">pybind11</a> and created an animated plot using <a href="https://matplotlib.org/">matplotlib</a> and <a href="https://pypi.org/project/plot2vid/">plot2vid</a>.</p> <figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*YHwPJFarsbnw9t4cvxN-ew.gif"/></figure> <p>Turns out that the technique I described is much more powerful than just ropes. You can easily transform this code to create more fun stuff, including cloths and solid bodies. For inspiration, check out <a href="https://jlpaca.github.io/toybox/6-jakobsen/index.html">this website</a>.</p> <h3>References and further reading</h3> <ol><li><a href="https://owlree.blog/posts/simulating-a-rope.html">Very detailed explanation of this algorithm</a></li><li><a href="https://jlpaca.github.io/toybox/6-jakobsen/index.html">Fun examples of Jakobsen Particle Simulation</a></li><li><a href="https://www.cs.cmu.edu/afs/cs/academic/class/15462-s13/www/lec_slides/Jakobsen.pdf">A comprehensive tutorial on Jakobsen Constraints by Jakobsen himself</a></li></ol> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a595a3ef956c" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>